{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "University of Artificial Intelligence. HomeWork 17.Pro.1. Text generation. Chat-bot",
      "provenance": [],
      "collapsed_sections": [
        "1g-tBeFP38Rp",
        "Zfhu067toXGQ"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rubygitflow/Sequence-to-Sequence-training-models/blob/master/University_of_Artificial_Intelligence_HomeWork_17_Pro_1_Text_generation_Chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-mydfzpnu6M",
        "colab_type": "text"
      },
      "source": [
        "# Lesson 17.Text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g-tBeFP38Rp",
        "colab_type": "text"
      },
      "source": [
        "# **Import библиотек**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3j1Wpkvc3Q2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files # модуль для загрузки файлов в colab\n",
        "import numpy as np #библиотека для работы с массивами данных\n",
        "\n",
        "from tensorflow.keras.models import Model, load_model # из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input # из кераса загружаем необходимые слои для нейросети\n",
        "from tensorflow.keras.optimizers import RMSprop, Adadelta # из кераса загружаем выбранный оптимизатор\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences # загружаем метод ограничения последовательности заданной длиной\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # загружаем токенизатор кераса для обработки текста\n",
        "from tensorflow.keras import utils # загружаем утилиты кераса для one hot кодировки\n",
        "from tensorflow.keras.utils import plot_model # удобный график для визуализации архитектуры модели\n",
        "\n",
        "import yaml # импортируем модуль для удобной работы с файлами"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "200dSPOYZE7N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxdi0Fqeg1LH",
        "colab_type": "text"
      },
      "source": [
        "# **Парсинг данных**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEA8TR_oerov",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "46076537-085d-44d4-a573-338ffbf3c44d"
      },
      "source": [
        "######################\n",
        "# Открываем файл с диалогами\n",
        "######################\n",
        "corpus = open('/content/drive/My Drive/Базы/Диалоги(рассказы).yml', 'r') # открываем файл с диалогами в режиме чтения\n",
        "document = yaml.safe_load(corpus) # загружаем файл *глоссарий\n",
        "conversations = document['разговоры'] # загружаем диалоги из файла и заносим в conversations \n",
        "print('Количество пар вопрос-ответ : {}'.format(len(conversations)))\n",
        "print('Пример диалога : {}'.format(conversations[200]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Количество пар вопрос-ответ : 11905\n",
            "Пример диалога : ['Около сотни...', 'Точнее!']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoQKgJwLe-w_",
        "colab_type": "text"
      },
      "source": [
        "**2) Учёт в тексте знаков препинания**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srWGYI66e7kf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def insert_space(text, index):\n",
        "  return text[:index] + ' ' + text[index:]\n",
        "\n",
        "######################\n",
        "# Парсим текстовую строку - выделяем пункутуацию, \n",
        "# сохраняем написание сокращений и действительных чисел\n",
        "######################\n",
        "def parsPunctuation(text):\n",
        "  # преобразование списочного типа данных\n",
        "  if type(text) == str:        # то передаём переменную  как есть\n",
        "    out_text = text            \n",
        "  elif type(text) == list:     # то преобразуем тип данных перед передачей переменной\n",
        "    out_text = ''\n",
        "    for elem in text: \n",
        "      out_text += ' ' + elem \n",
        "  else:                        # иначе обнуляем переменную\n",
        "    print('Early stop')\n",
        "    return \"\"\n",
        "\n",
        "  # выделение знаков препинания\n",
        "  out_text = out_text.replace('.-', '. -')\n",
        "\n",
        "  # обработка графематики \"точек\"\n",
        "  len_text = len(out_text)\n",
        "  if out_text.find('.') > -1:\n",
        "    text_dot = out_text\n",
        "    for j in reversed(range(len_text)):\n",
        "      if out_text[j]=='.':\n",
        "        # игнорируем инициалы и сокращения\n",
        "        if not ( (j > 2 and  out_text[j-2]=='.') or \\\n",
        "                (j < (len_text - 2) and out_text[j+2]=='.') or \\\n",
        "                (j > 2 and  out_text[j-2]==' ') or \\\n",
        "                (j > 2 and  out_text[j-3]==' ') ):\n",
        "          text_dot = insert_space(text_dot, j)\n",
        "  else:\n",
        "    text_dot = out_text\n",
        "\n",
        "  len_text = len(text_dot)\n",
        "          \n",
        "  # обработка графематики \"запятых\"\n",
        "  if text_dot.find(',') > -1:\n",
        "    text_comma = text_dot\n",
        "    for j in reversed(range(len_text)):\n",
        "      if text_dot[j]==',':\n",
        "        if not (j < (len_text - 1) and text_dot[j+1].isdigit()): # число через запятую\n",
        "          if j < (len_text - 1) and not text_dot[j+1] in [\" \", \"\\t\"]:\n",
        "            text_comma = insert_space(text_comma, j+1)\n",
        "          text_comma = insert_space(text_comma, j)\n",
        "  else:\n",
        "    text_comma = text_dot\n",
        "\n",
        "  # обработка графематики остальных знаков препинания\n",
        "  text_comma = text_comma.replace('!', ' !')\n",
        "  text_comma = text_comma.replace('?', ' ?')\n",
        "  text_comma = text_comma.replace(':', ' :')\n",
        "\n",
        "  text_comma = text_comma.replace('=', ' = ')\n",
        "  text_comma = text_comma.replace('/', ' / ')\n",
        "  text_comma = text_comma.replace('(', ' ( ')\n",
        "  text_comma = text_comma.replace(')', ' ) ')\n",
        "  text_comma = text_comma.replace('[', ' [ ')\n",
        "  text_comma = text_comma.replace(']', ' ] ')\n",
        "  text_comma = text_comma.replace('{', ' { ')\n",
        "  text_comma = text_comma.replace('}', ' } ')\n",
        "  text_comma = text_comma.replace('<', ' < ')\n",
        "  text_comma = text_comma.replace('>', ' > ')\n",
        "  text_comma = text_comma.replace('\"', ' \" ')\n",
        "  text_comma = text_comma.replace('„', ' „ ')\n",
        "  text_comma = text_comma.replace('“', ' “ ')\n",
        "  text_comma = text_comma.replace('«', ' « ')\n",
        "  text_comma = text_comma.replace('»', ' » ')\n",
        "  text_comma = text_comma.replace(';', ' ; ')\n",
        "  text_comma = text_comma.replace('\\xa0', '')\n",
        "  text_comma = text_comma.replace('\\x301', '')\n",
        "  text_comma = text_comma.replace('\\u0301', '')\n",
        "  text_comma = text_comma.replace('\\ufeff', '')\n",
        "            \n",
        "  text_comma = text_comma.replace('  ', ' ')\n",
        "  text_comma = text_comma.replace('  ', ' ')\n",
        "\n",
        "  text_comma = text_comma.strip()      \n",
        "\n",
        "  return text_comma"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYg8z8Vj76bu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "95d47f4c-0553-4bcf-f806-438a3597d458"
      },
      "source": [
        "######################\n",
        "# Разбираем вопросы-ответы с проставлением тегов ответам\n",
        "######################\n",
        "# Собираем вопросы и ответы в списки\n",
        "questions = list() # здесь будет список вопросов\n",
        "answers = list() # здесь будет список ответов\n",
        "\n",
        "# В каждом диалоге берем фразу и добавляем в лист\n",
        "# Если в ответе не одна фраза - то сцепляем сколько есть\n",
        "for i, con in enumerate(conversations): # для каждой пары вопрос-ответ\n",
        "  if i > 7000: # РЕШАЕМ ПРОБЛЕМУ НЕХВАТКИ ПАМЯТИ НА КОЛЛАБЕ\n",
        "    break\n",
        "  if len(con) > 2 : # если ответ содержит более двух предложений (кол-во реплик, кол-во вариантов ответа)\n",
        "    questions.append(parsPunctuation(con[0])) # то вопросительную реплику отправляем в список вопросов\n",
        "    replies = con[1:] # а ответную составляем из последующих строк\n",
        "    ans = '' # здесь соберем ответ\n",
        "    for rep in replies: # каждую реплику в ответной реплике\n",
        "      ans += ' ' + rep \n",
        "    answers.append(ans) #добавим в список ответов\n",
        "  elif len(con)> 1: # если на 1 вопрос приходится 1 ответ\n",
        "    questions.append(parsPunctuation(con[0])) # то вопросительную реплику отправляем в список вопросов\n",
        "    answers.append(con[1]) # а ответную в список ответов\n",
        "\n",
        "# Очищаем строки с неопределенным типов ответов\n",
        "answersCleaned = list()\n",
        "for i in range(len(answers)):\n",
        "  if type(answers[i]) == str:\n",
        "    answersCleaned.append(parsPunctuation(answers[i])) #если тип - строка, то добавляем в ответы\n",
        "  else:\n",
        "    questions.pop(i) # если не строка, то ответ не добавился, и плюс убираем соответствующий вопрос\n",
        "\n",
        "# Сделаем теги-метки для начала и конца ответов\n",
        "answers = list()\n",
        "for i in range(len(answersCleaned)):\n",
        "  answers.append( '<START> ' + answersCleaned[i] + ' <END>' )\n",
        "\n",
        "# Выведем обновленные данные на экран\n",
        "print('Вопрос : {}'.format(questions[200]))\n",
        "print('Ответ : {}'.format(answers[200]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Вопрос : Около сотни. ..\n",
            "Ответ : <START> Точнее ! <END>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5viaf_Yf487",
        "colab_type": "text"
      },
      "source": [
        "**1.1) Разрешить обработку unknown слов**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvn1jvRd9tep",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "6db826d2-9116-4fd6-c68f-14444a1eb6a9"
      },
      "source": [
        "######################\n",
        "# Подключаем токенизатор от keras и собираем словарь индексов\n",
        "######################\n",
        "tokenizer = Tokenizer(filters='<>', oov_token='unknown')\n",
        "tokenizer.fit_on_texts(questions + answers) # загружаем в токенизатор список вопросов-ответов для сборки словаря частотности\n",
        "vocabularyItems = list(tokenizer.word_index.items()) # список с cодержимым словаря\n",
        "vocabularySize = len(vocabularyItems)+1 # размер словаря\n",
        "print( 'Фрагмент словаря : {}'.format(vocabularyItems[:50]))\n",
        "print( 'Размер словаря : {}'.format(vocabularySize))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Фрагмент словаря : [('unknown', 1), ('start', 2), ('end', 3), ('.', 4), ('?', 5), (',', 6), ('!', 7), ('не', 8), ('что', 9), ('а', 10), ('ты', 11), ('я', 12), ('..', 13), ('это', 14), ('в', 15), ('как', 16), ('да', 17), ('и', 18), ('нет', 19), ('вы', 20), ('ну', 21), ('на', 22), ('с', 23), ('же', 24), ('так', 25), ('у', 26), ('где', 27), ('кто', 28), ('он', 29), ('-', 30), ('все', 31), ('тебя', 32), ('мне', 33), ('мы', 34), ('\"', 35), ('меня', 36), ('почему', 37), ('куда', 38), ('есть', 39), ('там', 40), ('вот', 41), ('тебе', 42), ('за', 43), ('еще', 44), ('ничего', 45), ('здесь', 46), ('знаю', 47), ('его', 48), ('вас', 49), ('ли', 50)]\n",
            "Размер словаря : 11012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsUqzEBXg9Mu",
        "colab_type": "text"
      },
      "source": [
        "# **Подготовка выборки**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4nNBJUQgebF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "4063decd-a1ed-44c8-87e0-ee3eaa4be3ec"
      },
      "source": [
        "######################\n",
        "# Устанавливаем закодированные входные данные(вопросы)\n",
        "######################\n",
        "tokenizedQuestions = tokenizer.texts_to_sequences(questions) # разбиваем текст вопросов на последовательности индексов\n",
        "maxLenQuestions = max([ len(x) for x in tokenizedQuestions]) # уточняем длину самого большого вопроса\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие вопросы\n",
        "paddedQuestions = pad_sequences(tokenizedQuestions, maxlen=maxLenQuestions, padding='post')\n",
        "\n",
        "# Предподготавливаем данные для входа в сеть\n",
        "encoderForInput = np.array(paddedQuestions) # переводим в numpy массив\n",
        "print('Пример оригинального вопроса на вход : {}'.format(questions[100])) \n",
        "print('Пример кодированного вопроса на вход : {}'.format(encoderForInput[100])) \n",
        "print('Размеры закодированного массива вопросов на вход : {}'.format(encoderForInput.shape)) \n",
        "print('Установленная длина вопросов на вход : {}'.format(maxLenQuestions)) \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пример оригинального вопроса на вход : Какая же мораль ?\n",
            "Пример кодированного вопроса на вход : [ 179   24 3895    5    0    0    0    0    0    0    0    0    0    0\n",
            "    0]\n",
            "Размеры закодированного массива вопросов на вход : (6998, 15)\n",
            "Установленная длина вопросов на вход : 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tjvhMuzqFJD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "f4e65a32-e6ae-4cbc-f5ed-f920047820a5"
      },
      "source": [
        "######################\n",
        "# Устанавливаем раскодированные входные данные(ответы)\n",
        "######################\n",
        "tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов\n",
        "maxLenAnswers = max([len(x) for x in tokenizedAnswers]) # уточняем длину самого большого ответа\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие ответы\n",
        "paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers, padding='post')\n",
        "\n",
        "# Предподготавливаем данные для входа в сеть\n",
        "decoderForInput = np.array(paddedAnswers) # переводим в numpy массив\n",
        "print('Пример оригинального ответа на вход: {}'.format(answers[100])) \n",
        "print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[100][:30])) \n",
        "print('Размеры раскодированного массива ответов на вход : {}'.format(decoderForInput.shape)) \n",
        "print('Установленная длина ответов на вход : {}'.format(maxLenAnswers)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пример оригинального ответа на вход: <START> Никакой . Так просто вспомнилось . <END>\n",
            "Пример раскодированного ответа на вход : [   2  707    4   25  107 7521    4    3    0    0    0    0    0    0\n",
            "    0    0    0]\n",
            "Размеры раскодированного массива ответов на вход : (6998, 17)\n",
            "Установленная длина ответов на вход : 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwsKk9dzNeqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################\n",
        "# Раскодированные выходные данные(ответы)\n",
        "######################\n",
        "tokenizedAnswers = tokenizer.texts_to_sequences(answers) # разбиваем текст ответов на последовательности индексов\n",
        "for i in range(len(tokenizedAnswers)) : # для разбитых на последовательности ответов\n",
        "  tokenizedAnswers[i] = tokenizedAnswers[i][1:] # избавляемся от тега <START>\n",
        "# Делаем последовательности одной длины, заполняя нулями более короткие ответы\n",
        "paddedAnswers = pad_sequences(tokenizedAnswers, maxlen=maxLenAnswers , padding='post')\n",
        "\n",
        "oneHotAnswers = utils.to_categorical(paddedAnswers, vocabularySize) # переводим в one hot vector\n",
        "decoderForOutput = np.array(oneHotAnswers) # и сохраняем в виде массива numpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRl1k7SVaA6w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "33e324e7-2a19-455a-f6b2-5864adef49c7"
      },
      "source": [
        "print('Пример раскодированного ответа на вход : {}'.format(decoderForInput[100][:21]))  \n",
        "print('Пример раскодированного ответа на выход : {}'.format(decoderForOutput[100][1][:21])) \n",
        "print('Размеры раскодированного массива ответов на выход : {}'.format(decoderForOutput.shape))\n",
        "print('Установленная длина вопросов на выход : {}'.format(maxLenAnswers)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Пример раскодированного ответа на вход : [   2  707    4   25  107 7521    4    3    0    0    0    0    0    0\n",
            "    0    0    0]\n",
            "Пример раскодированного ответа на выход : [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Размеры раскодированного массива ответов на выход : (6998, 17, 11012)\n",
            "Установленная длина вопросов на выход : 17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0KR6Mh_hp1f",
        "colab_type": "text"
      },
      "source": [
        "# **Параметры нейросети и модель обучения**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rRKDr4rhXcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################\n",
        "# Первый входной слой, кодер, выходной слой\n",
        "######################\n",
        "encoderInputs = Input(shape=(None , )) # размеры на входе сетки (здесь будет encoderForInput)\n",
        "# Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
        "encoderEmbedding = Embedding(vocabularySize, 200 , mask_zero=True) (encoderInputs)\n",
        "# Затем выход с Embedding пойдёт в LSTM слой, на выходе у которого будет два вектора состояния - state_h , state_c\n",
        "# Вектора состояния - state_h , state_c зададутся в LSTM слое декодера в блоке ниже\n",
        "encoderOutputs, state_h , state_c = LSTM(200, return_state=True)(encoderEmbedding)\n",
        "encoderStates = [state_h, state_c]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_yv8Y6QWX2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################\n",
        "# Второй входной слой, декодер, выходной слой\n",
        "######################\n",
        "decoderInputs = Input(shape=(None, )) # размеры на входе сетки (здесь будет decoderForInput)\n",
        "# Эти данные проходят через слой Embedding (длина словаря, размерность) \n",
        "# mask_zero=True - игнорировать нулевые padding при передаче в LSTM. Предотвратит вывод ответа типа: \"У меня все хорошо PAD PAD PAD PAD PAD PAD..\"\n",
        "decoderEmbedding = Embedding(vocabularySize, 200, mask_zero=True) (decoderInputs) \n",
        "# Затем выход с Embedding пойдёт в LSTM слой, которому передаются вектора состояния - state_h , state_c\n",
        "decoderLSTM = LSTM(200, return_state=True, return_sequences=True)\n",
        "decoderOutputs , _ , _ = decoderLSTM (decoderEmbedding, initial_state=encoderStates)\n",
        "# И от LSTM'а сигнал decoderOutputs пропускаем через полносвязный слой с софтмаксом на выходе\n",
        "decoderDense = Dense(vocabularySize, activation='softmax') \n",
        "output = decoderDense (decoderOutputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYnTen_UWc5F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "05f073e0-cbea-45e5-9ea6-ed4fa683ea21"
      },
      "source": [
        "######################\n",
        "# Собираем тренировочную модель нейросети\n",
        "######################\n",
        "model = Model([encoderInputs, decoderInputs], output)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy')\n",
        "\n",
        "print(model.summary()) # выведем на экран информацию о построенной модели нейросети\n",
        "plot_model(model, to_file='model.png') # и построим график для визуализации слоев и связей между ними"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_12 (InputLayer)           [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 200)    2202400     input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "embedding_3 (Embedding)         (None, None, 200)    2202400     input_12[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, 200), (None, 320800      embedding_2[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_3 (LSTM)                   [(None, None, 200),  320800      embedding_3[0][0]                \n",
            "                                                                 lstm_2[0][1]                     \n",
            "                                                                 lstm_2[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 11012)  2213412     lstm_3[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 7,259,812\n",
            "Trainable params: 7,259,812\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbelEm0zhadD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebc31234-73d0-4804-a1aa-dff440d23e19"
      },
      "source": [
        "# Запустим обучение и сохраним модель\n",
        "model.fit([encoderForInput , decoderForInput], decoderForOutput, batch_size=50, epochs=30) \n",
        "model.save( '/content/drive/My Drive/tmp/model_30epochs(rms).h5' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 2.0562\n",
            "Epoch 2/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.7016\n",
            "Epoch 3/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.6280\n",
            "Epoch 4/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.5733\n",
            "Epoch 5/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.5321\n",
            "Epoch 6/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 1.4954\n",
            "Epoch 7/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.4599\n",
            "Epoch 8/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 1.4258\n",
            "Epoch 9/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 1.3918\n",
            "Epoch 10/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.3592\n",
            "Epoch 11/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.3263\n",
            "Epoch 12/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 1.2944\n",
            "Epoch 13/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.2638\n",
            "Epoch 14/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 1.2372\n",
            "Epoch 15/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 1.2101\n",
            "Epoch 16/30\n",
            "140/140 [==============================] - 6s 46ms/step - loss: 1.1832\n",
            "Epoch 17/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 1.1570\n",
            "Epoch 18/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.1328\n",
            "Epoch 19/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.1093\n",
            "Epoch 20/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.0866\n",
            "Epoch 21/30\n",
            "140/140 [==============================] - 6s 46ms/step - loss: 1.0651\n",
            "Epoch 22/30\n",
            "140/140 [==============================] - 6s 46ms/step - loss: 1.0431\n",
            "Epoch 23/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 1.0205\n",
            "Epoch 24/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 0.9979\n",
            "Epoch 25/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 0.9741\n",
            "Epoch 26/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 0.9517\n",
            "Epoch 27/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 0.9294\n",
            "Epoch 28/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 0.9092\n",
            "Epoch 29/30\n",
            "140/140 [==============================] - 6s 45ms/step - loss: 0.8891\n",
            "Epoch 30/30\n",
            "140/140 [==============================] - 6s 44ms/step - loss: 0.8687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_U_rY8UiRL2",
        "colab_type": "text"
      },
      "source": [
        "# **Подготовка и запуск рабочей нейросети с генерацией ответов**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv9utvcjh2co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################\n",
        "# Создаем рабочую модель для вывода ответов на запросы пользователя\n",
        "######################\n",
        "def makeInferenceModels():\n",
        "  # Определим модель кодера, на входе далее будут закодированные вопросы(encoderForInputs), на выходе состояния state_h, state_c\n",
        "  encoderModel = Model(encoderInputs, encoderStates) \n",
        "\n",
        "  decoderStateInput_h = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_h\n",
        "  decoderStateInput_c = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_c\n",
        "\n",
        "  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] # возьмем оба inputs вместе и запишем в decoderStatesInputs\n",
        "\n",
        "  # Берём ответы, прошедшие через эмбединг, вместе с состояниями и подаём LSTM cлою\n",
        "  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding, initial_state=decoderStatesInputs)\n",
        "  decoderStates = [state_h, state_c] # LSTM даст нам новые состояния\n",
        "  decoderOutputs = decoderDense(decoderOutputs) # и ответы, которые мы пропустим через полносвязный слой с софтмаксом\n",
        "\n",
        "  # Определим модель декодера, на входе далее будут раскодированные ответы (decoderForInputs) и состояния\n",
        "  # на выходе предсказываемый ответ и новые состояния\n",
        "  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)\n",
        "\n",
        "  return encoderModel , decoderModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "prqyy840HhLx"
      },
      "source": [
        "**1.2) Разрешить обработку unknown слов**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSSOhZpgh9LI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################\n",
        "# Создадим функцию, которая преобразует вопрос пользователя в последовательность индексов\n",
        "######################\n",
        "def strToTokens(sentence: str): # функция принимает строку на вход (предложение с вопросом)\n",
        "  checkIndex = tokenizer.texts_to_sequences([sentence]) # получаем индексы для каждого слова\n",
        "  words = parsPunctuation(sentence).lower().split() # приводит предложение к нижнему регистру и разбирает на слова\n",
        "  tokensList = list() # здесь будет последовательность токенов/индексов\n",
        "  for index in range(len(checkIndex[0])): # проходимся по длине списка индексов\n",
        "    if checkIndex[0][index]!=1: # если слово не unknown (имеет индекс 1)\n",
        "      try:\n",
        "        tokensList.append(tokenizer.word_index[words[index]]) # определяем токенизатором индекс и добавляем в список\n",
        "      except Exception:\n",
        "        print('Что-то пошло не так, раз. Здесь нужно подумать над алгоритмом')\n",
        "\n",
        "    # Функция вернёт вопрос в виде последовательности индексов, ограниченной длиной самого длинного вопроса из нашей базы вопросов\n",
        "  return pad_sequences([tokensList], maxlen=maxLenQuestions , padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ0Dxd1eiEid",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "4360cda2-43aa-477a-cea2-df413ad847cd"
      },
      "source": [
        "######################\n",
        "# Устанавливаем окончательные настройки и запускаем модель\n",
        "######################\n",
        "\n",
        "encModel , decModel = makeInferenceModels() # запускаем функцию для построения модели кодера и декодера\n",
        "\n",
        "for _ in range(6): # задаем количество вопросов, и на каждой итерации в этом диапазоне:\n",
        "  # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом\n",
        "  try:\n",
        "    statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))\n",
        "  except Exception:\n",
        "    print('Что-то пошло не так, два. Здесь нужно подумать над алгоритмом')\n",
        "  # Создаём пустой массив размером (1, 1)\n",
        "  emptyTargetSeq = np.zeros((1, 1))    \n",
        "  emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса\n",
        "\n",
        "  stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова\n",
        "  decodedTranslation = '' # здесь будет собираться генерируемый ответ\n",
        "  while not stopCondition : # пока не сработало стоп-условие\n",
        "    # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.\n",
        "    # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния\n",
        "    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "    \n",
        "    #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве\n",
        "    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.\n",
        "    sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык\n",
        "    for word , index in tokenizer.word_index.items():\n",
        "      if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря\n",
        "        decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ \n",
        "        sampledWord = word # выбранное слово фиксируем в переменную sampledWord\n",
        "    \n",
        "    # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа\n",
        "    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:\n",
        "      stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию\n",
        "\n",
        "    emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив\n",
        "    emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова\n",
        "    statesValues = [h, c] # и состояния, обновленные декодером\n",
        "    # и продолжаем цикл с обновленными параметрами\n",
        "  \n",
        "  print(decodedTranslation) # выводим ответ сгенерированный декодером"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Задайте вопрос : Привет, машина.\n",
            "Что-то пошло не так, два. Здесь нужно подумать над алгоритмом\n",
            " ! end\n",
            "Задайте вопрос : Привет машина.\n",
            " , товарищ генерал . end\n",
            "Задайте вопрос : Работа зовёт, как думаешь?\n",
            " , товарищ генерал . end\n",
            "Задайте вопрос : машина\n",
            " , товарищ командир . end\n",
            "Задайте вопрос : А меня знаешь как зовут?\n",
            " . end\n",
            "Задайте вопрос : Так и будешь молчать?\n",
            " и не знаю . end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zfhu067toXGQ",
        "colab_type": "text"
      },
      "source": [
        "# **Загрузка и запуск предобученной модели**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ngvBy5yrY3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8c20373a-aacb-49ac-c59c-b52ff7b81200"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Lg3eOVqKyP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "eab27138-53cb-4d74-d85e-e25a12de2d75"
      },
      "source": [
        "# Подгружаем модель из файла и выведем её параметры\n",
        "model = load_model('/content/gdrive/My Drive/Предобученные сети/model_chatbot_100epochs(rms)+50(ada).h5')\n",
        "#model = load_model('model_chatbot_100epochs(rms)+50(ada) (1).h5')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_1 (Embedding)         (None, None, 200)    3020800     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_2 (Embedding)         (None, None, 200)    3020800     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 200), (None, 320800      embedding_1[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "lstm_2 (LSTM)                   [(None, None, 200),  320800      embedding_2[0][0]                \n",
            "                                                                 lstm_1[0][1]                     \n",
            "                                                                 lstm_1[0][2]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, None, 15104)  3035904     lstm_2[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 9,719,104\n",
            "Trainable params: 9,719,104\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEfb58cXqsKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################\n",
        "# Устанавливаем связи между слоями рабочей модели и предобученной\n",
        "######################\n",
        "def loadInferenceModels():\n",
        "  encoderInputs = model.input[0]   # входом энкодера рабочей модели будет первый инпут предобученной модели(input_1)\n",
        "  encoderEmbedding = model.layers[2] # связываем эмбединг слои(model.layers[2] это embedding_1)\n",
        "  encoderOutputs, state_h_enc, state_c_enc = model.layers[4].output # вытягиваем аутпуты из первого LSTM слоя обуч.модели и даем энкодеру(lstm_1)\n",
        "  encoderStates = [state_h_enc, state_c_enc] # ложим забранные состояния в состояния энкодера\n",
        "  encoderModel = Model(encoderInputs, encoderStates) # формируем модель\n",
        "\n",
        "  decoderInputs = model.input[1]   # входом декодера рабочей модели будет второй инпут предобученной модели(input_2)\n",
        "  decoderStateInput_h = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_h\n",
        "  decoderStateInput_c = Input(shape=(200 ,)) # обозначим размерность для входного слоя с состоянием state_c\n",
        "\n",
        "  decoderStatesInputs = [decoderStateInput_h, decoderStateInput_c] # возьмем оба inputs вместе и запишем в decoderStatesInputs\n",
        "\n",
        "  decoderEmbedding = model.layers[3] # связываем эмбединг слои(model.layers[3] это embedding_2)\n",
        "  decoderLSTM = model.layers[5] # связываем LSTM слои(model.layers[5] это lstm_2)\n",
        "  decoderOutputs, state_h, state_c = decoderLSTM(decoderEmbedding.output, initial_state=decoderStatesInputs)\n",
        "  decoderStates = [state_h, state_c] # LSTM даст нам новые состояния\n",
        "\n",
        "  decoderDense = model.layers[6] # связываем полносвязные слои(model.layers[6] это dense_1)\n",
        "  decoderOutputs = decoderDense(decoderOutputs) # выход с LSTM мы пропустим через полносвязный слой с софтмаксом\n",
        "\n",
        "    # Определим модель декодера, на входе далее будут раскодированные ответы (decoderForInputs) и состояния\n",
        "    # на выходе предсказываемый ответ и новые состояния\n",
        "  decoderModel = Model([decoderInputs] + decoderStatesInputs, [decoderOutputs] + decoderStates)\n",
        "  return encoderModel , decoderModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTpsqjakx2rs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "52555a95-28de-4060-82a0-914c720a3cee"
      },
      "source": [
        "######################\n",
        "# Устанавливаем окончательные настройки и запускаем рабочую модель над предобученной\n",
        "######################\n",
        "\n",
        "encModel , decModel = loadInferenceModels() # запускаем функцию для построения модели кодера и декодера\n",
        "\n",
        "for _ in range(6): # задаем количество вопросов, и на каждой итерации в этом диапазоне:\n",
        "  # Получаем значения состояний, которые определит кодер в соответствии с заданным вопросом\n",
        "  statesValues = encModel.predict(strToTokens(input( 'Задайте вопрос : ' )))\n",
        "  # Создаём пустой массив размером (1, 1)\n",
        "  emptyTargetSeq = np.zeros((1, 1))    \n",
        "  emptyTargetSeq[0, 0] = tokenizer.word_index['start'] # положим в пустую последовательность начальное слово 'start' в виде индекса\n",
        "\n",
        "  stopCondition = False # зададим условие, при срабатывании которого, прекратится генерация очередного слова\n",
        "  decodedTranslation = '' # здесь будет собираться генерируемый ответ\n",
        "  while not stopCondition : # пока не сработало стоп-условие\n",
        "    # В модель декодера подадим пустую последовательность со словом 'start' и состояния предсказанные кодером по заданному вопросу.\n",
        "    # декодер заменит слово 'start' предсказанным сгенерированным словом и обновит состояния\n",
        "    decOutputs , h , c = decModel.predict([emptyTargetSeq] + statesValues)\n",
        "    \n",
        "    #argmax пробежит по вектору decOutputs'а[0,0,15104], найдет макс.значение, и вернёт нам номер индекса под которым оно лежит в массиве\n",
        "    sampledWordIndex = np.argmax( decOutputs[0, 0, :]) # argmax возьмем от оси, в которой 15104 элементов. Получили индекс предсказанного слова.\n",
        "    sampledWord = None # создаем переменную, в которую положим слово, преобразованное на естественный язык\n",
        "    for word , index in tokenizer.word_index.items():\n",
        "      if sampledWordIndex == index: # если индекс выбранного слова соответствует какому-то индексу из словаря\n",
        "        decodedTranslation += ' {}'.format(word) # слово, идущее под этим индексом в словаре, добавляется в итоговый ответ \n",
        "        sampledWord = word # выбранное слово фиксируем в переменную sampledWord\n",
        "    \n",
        "    # Если выбранным словом оказывается 'end' либо если сгенерированный ответ превышает заданную максимальную длину ответа\n",
        "    if sampledWord == 'end' or len(decodedTranslation.split()) > maxLenAnswers:\n",
        "      stopCondition = True # то срабатывает стоп-условие и прекращаем генерацию\n",
        "\n",
        "    emptyTargetSeq = np.zeros((1, 1)) # создаем пустой массив\n",
        "    emptyTargetSeq[0, 0] = sampledWordIndex # заносим туда индекс выбранного слова\n",
        "    statesValues = [h, c] # и состояния, обновленные декодером\n",
        "    # и продолжаем цикл с обновленными параметрами\n",
        "  \n",
        "  print(decodedTranslation) # выводим ответ сгенерированный декодером"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Задайте вопрос : ты кто \n",
            " наш вид известен как люди end\n",
            "Задайте вопрос : скажи что нибудь \n",
            " а что надо говорить end\n",
            "Задайте вопрос : какой сегодня день\n",
            " четверг end\n",
            "Задайте вопрос : сколько времени\n",
            " без пяти пять end\n",
            "Задайте вопрос : как настроение\n",
            " но так то и не знаю end\n",
            "Задайте вопрос : что то случилось\n",
            " о end\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98O0XcM9yCbm",
        "colab_type": "text"
      },
      "source": [
        "# **Глоссарий**\n",
        "-  Seq2Seq - sequence-to-sequence модель, состоит из двух рекуррентных нейронных сетей (RNN): \n",
        "\n",
        "encoder (кодер), которая обрабатывает входные данные,\n",
        "\n",
        "decoder (декодер), которая генерирует данные вывода.\n",
        "- Yaml - удобный текстовый формат, позволяющий хранить структурированные данные в иерархии. https://ru.bmstu.wiki/YAML\n",
        "yaml.safe_load - безопасный метод загрузки данных из файлов, предотвращающий возможность запуска произвольного кода для файлов из ненадежных источников\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}